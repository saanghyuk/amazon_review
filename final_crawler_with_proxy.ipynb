{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Search Query\n",
    "SEARCH_QUERY = 'tv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY_CRAWL_TOKEN = '8wJqpL2FErGiPGX7QWP6Aw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy(url):\n",
    "    quoted_url = quote(url)\n",
    "    res = requests.get(f'https://api.proxycrawl.com/?token={PROXY_CRAWL_TOKEN}&url={quoted_url}')\n",
    "    res.raise_for_status() # Raise error if it fails\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A function to get the content of the page of required query\n",
    "def search_in_amazon(search_query):\n",
    "    url = f\"https://www.amazon.com/s?k={search_query}\"\n",
    "    return get_proxy(url)\n",
    "\n",
    "#### A function to get the contents of individual product pages using 'data-asin' number (unique identification number)\n",
    "def search_asin(asin):\n",
    "    url = f\"https://www.amazon.com/dp/{asin}\"\n",
    "    return get_proxy(url)\n",
    "\n",
    "#### A function to pass on the link of 'see all reviews' and extract the content\n",
    "def search_reviews(review_link):\n",
    "    url = \"https://www.amazon.com{review_link}\"\n",
    "    return get_proxy(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Name extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start to extract product names\")\n",
    "\n",
    "product_names = []\n",
    "data_asin = []\n",
    "LAST_PAGE = 21\n",
    "for i in range(1,LAST_PAGE):\n",
    "    print(f\"Iteration {i}/{LAST_PAGE}\")\n",
    "    html = search_in_amazon(SEARCH_QUERY+'&page='+str(i))\n",
    "    soup = BeautifulSoup(html)\n",
    "    for i in soup.findAll(\"span\",{'class':'a-size-medium a-color-base a-text-normal'}):\n",
    "        product_names.append(i.text) # adding the product names to the list\n",
    "\n",
    "    for i in soup.findAll(\"div\", {\"class\":\"s-result-item\"}):\n",
    "        if i['data-asin']:\n",
    "            data_asin.append(i['data-asin'])\n",
    "\n",
    "print(\"Finished extract product names\")\n",
    "print(product_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scrawling the all pages of product list in specific search query, I could discover that there are same products in the list. <br>\n",
    "Therefore, I needed to remove the same product in the list of ASIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_asin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ac5d3147ce2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masin_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_asin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_asin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masin_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_asin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_asin' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "asin_list = list(dict.fromkeys(data_asin) )\n",
    "data_asin = asin_list\n",
    "\n",
    "link=[]\n",
    "for i in range(len(data_asin)):\n",
    "    print(i)\n",
    "    html = search_asin(data_asin[i])\n",
    "    soup = BeautifulSoup(html)\n",
    "    for i in soup.findAll(\"a\",{'data-hook':\"see-all-reviews-link-foot\"}):\n",
    "        print(i['href'])\n",
    "        link.append(i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_list = []\n",
    "reviews=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for j in range(len(link)):\n",
    "    print(j, 'th started')\n",
    "    for k in range(1, 2):\n",
    "        html = search_reviews(link[j]+'&pageNumber='+str(k))\n",
    "        soup = BeautifulSoup(html)\n",
    "        if soup.find('div',\n",
    "                     {\"class\" : \"a-section a-spacing-top-large a-text-center no-reviews-section\"}):\n",
    "            print('No review, Pass')\n",
    "            break\n",
    "        else:\n",
    "            print('There is review')\n",
    "#           for i in soup.findAll(\"span\",{'data-hook':\"review-body\"}):\n",
    "            for i in soup.findAll(\"span\",{'data-hook':\"a-size-base review-text review-text-content\"}):\n",
    "                reviews.append(i.text)\n",
    "                search_query_list.append(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev={'search_query':search_query_list, 'reviews' :reviews} #converting the reviews list into a dictionary\n",
    "review_data=pd.DataFrame.from_dict(rev) #converting this dictionary into a dataframe\n",
    "\n",
    "df = review_data.replace('\\n','', regex=True)\n",
    "\n",
    "writer= pd.ExcelWriter(SEARCH_QUERY+'_review.xlsx')\n",
    "df.to_excel(writer, 'Sheet1', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
